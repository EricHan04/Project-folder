One of Joy Buolamwini's classes in computer vision technology at MIT required students to pick something they fancied from science fiction and try to make it real. Joy Buolamwini is an African-American woman. Her fun idea was to make an "aspiration mirror" that would read her face digitally and turn it into whatever she felt like that morning.
For example, she says in Shalini Kantayya's documentary Coded Bias - screening as part of the Women in Film festival - she liked the idea of merging her face with a lion's face. But the algorithm, as she was to discover, was geared to read the faces of white men. "It didn't work," she says, "until I put on a white mask."
It was a wake-up moment that led the ebullient Buolamwini down a long rabbit-hole investigating other ways in which computer technologies have been programmed, often unconsciously, to discriminate against whoever doesn't fit their in-built assumptions. Kantayya sticks with her, increasingly aghast at what she and other women researching the field discover, all the way to a Congressional hearing about a sector that in the United States is entirely unregulated.
Kantayya was "terrified and shocked" by what she found out. "I didn't really understand the film I was making when I started," she admits. She has previously made films about water privatisation and solar energy in working-class communities; she is interested in how disruptive technologies affect marginalised people. "But my films never start with issues; they always start with compelling characters."
Buolamwini joins forces with a blue-haired Harvard mathematician called Cathy O'Neil; they are good company. Even so, Kantayya was keenly aware of making a potentially dry subject entertaining. "I would be at parties and people ask what I was doing and I would watch their eyes glaze over as I said I was making a film about racist robots," she says. "So I looked to the tropes of science fiction to help us understand it."
Some early examples of the sexist, racist ghosts in the machine are so absurd they're funny. Apple co-founder Steve Wozniak tweets that his wife has been offered a credit limit only a tenth of his, even though all their accounts are shared. An attempt by Amazon to remove human bias from judging employment applications is found to have immediately eliminated all female applicants - most because they put down that they played women's soccer.
So that bad idea was dropped, but plenty weren't. "Artificial intelligence systems are being deployed at massive scale to make important decisions, like who gets hired, who gets health care, who gets into college and how long a prison sentence someone serves," says Kantayya. We see the data fed into recommendations for prison sentences, including the prisoner's postcode and whether their parents had been jailed. It is obvious where those numbers lead.
Even the programmers don't know what decisions their creations are making. When it comes to surveillance, however, their over-reach is obvious. One of the most chilling moments in Coded Bias comes when a young school boy is identified by facial recognition technology as a known criminal. Crying, he is surrounded by police and interrogated before being rescued by civil liberties activists. He is only 14.
"I watched it like 30 times and my goosebumps didn't stop," says Kantayya. "For us as a film team to catch that was extraordinary, but this is happening all over."
In South Australia, a phone app using facial recognition technology combined with a GPS tracking system is to be introduced to monitor people in quarantine. Local civil liberties groups have urged more control on the use of biometrics; Coded Bias certainly brings their case home. "These companies have the type of data that make the Stasi look like they had a light touch," says Kantayya. "You can see easily how this data could be used for some of the worst abuses of civil and human rights."
Women in Film, Sydney Opera House, March 7.
CREDIT: Stephanie Bunbury
Word count: 663
