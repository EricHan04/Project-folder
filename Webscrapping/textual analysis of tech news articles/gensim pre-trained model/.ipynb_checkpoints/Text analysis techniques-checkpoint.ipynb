{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T23:51:39.082501Z",
     "start_time": "2023-11-28T23:51:37.658433Z"
    }
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import gensim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T23:51:39.209661Z",
     "start_time": "2023-11-28T23:51:39.082501Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\EricHan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\EricHan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with LDA - Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T23:52:01.323683Z",
     "start_time": "2023-11-28T23:51:39.209661Z"
    }
   },
   "outputs": [],
   "source": [
    "autonomous_vehicle = wikipedia.page(\"Autonomous Vehicle\")\n",
    "artificial_intelligence = wikipedia.page(\"Artificial Intelligence\")\n",
    "satellite = wikipedia.page(\"Satellite\")\n",
    "eiffel_tower = wikipedia.page(\"Eiffel Tower\")\n",
    "\n",
    "corpus = [autonomous_vehicle.content, artificial_intelligence.content, satellite.content, eiffel_tower.content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T23:52:01.333795Z",
     "start_time": "2023-11-28T23:52:01.323683Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(document):\n",
    "        # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        tokens = document.split()\n",
    "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in en_stop]\n",
    "        tokens = [word for word in tokens if len(word)  > 5]\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T23:52:02.242987Z",
     "start_time": "2023-11-28T23:52:01.335772Z"
    }
   },
   "outputs": [],
   "source": [
    "#iterate through the corpus list that contains the four Wikipedia articles and preprocess text\n",
    "processed_data = [];\n",
    "for doc in corpus:\n",
    "    tokens = preprocess_text(doc)\n",
    "    processed_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T23:52:02.255765Z",
     "start_time": "2023-11-28T23:52:02.242987Z"
    }
   },
   "outputs": [],
   "source": [
    "#use this list to create a dictionary and corresponding bag of words corpus\n",
    "gensim_dictionary = corpora.Dictionary(processed_data)\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(token, allow_update=True) for token in processed_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T23:52:02.262854Z",
     "start_time": "2023-11-28T23:52:02.256757Z"
    }
   },
   "outputs": [],
   "source": [
    "#save our dictionary as well as the bag of words corpus using pickle\n",
    "pickle.dump(gensim_corpus, open('gensim_corpus_corpus.pkl', 'wb'))\n",
    "gensim_dictionary.save('gensim_dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T23:52:02.686404Z",
     "start_time": "2023-11-28T23:52:02.262854Z"
    }
   },
   "outputs": [],
   "source": [
    "#create LDA model in Gensim, use the LdaModel class, pass the bag of words corpus to the LdaModel constructor\n",
    "lda_model = gensim.models.ldamodel.LdaModel(gensim_corpus, num_topics=10, id2word=gensim_dictionary, passes=20)\n",
    "lda_model.save('gensim_model.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T23:52:02.694051Z",
     "start_time": "2023-11-28T23:52:02.689646Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.093*\"satellite\" + 0.009*\"launch\" + 0.009*\"united\" + 0.009*\"communication\" + 0.009*\"launched\"')\n",
      "(1, '0.000*\"satellite\" + 0.000*\"intelligence\" + 0.000*\"artificial\" + 0.000*\"learning\" + 0.000*\"problem\"')\n",
      "(2, '0.016*\"intelligence\" + 0.015*\"machine\" + 0.014*\"learning\" + 0.014*\"problem\" + 0.012*\"artificial\"')\n",
      "(3, '0.000*\"satellite\" + 0.000*\"cruise\" + 0.000*\"vehicle\" + 0.000*\"eiffel\" + 0.000*\"artificial\"')\n",
      "(4, '0.000*\"cruise\" + 0.000*\"problem\" + 0.000*\"satellite\" + 0.000*\"learning\" + 0.000*\"machine\"')\n",
      "(5, '0.000*\"satellite\" + 0.000*\"intelligence\" + 0.000*\"learning\" + 0.000*\"artificial\" + 0.000*\"machine\"')\n",
      "(6, '0.069*\"cruise\" + 0.030*\"vehicle\" + 0.014*\"francisco\" + 0.012*\"driving\" + 0.011*\"autonomous\"')\n",
      "(7, '0.032*\"eiffel\" + 0.010*\"second\" + 0.009*\"french\" + 0.007*\"structure\" + 0.007*\"exposition\"')\n",
      "(8, '0.000*\"satellite\" + 0.000*\"eiffel\" + 0.000*\"cruise\" + 0.000*\"machine\" + 0.000*\"vehicle\"')\n",
      "(9, '0.000*\"cruise\" + 0.000*\"eiffel\" + 0.000*\"vehicle\" + 0.000*\"satellite\" + 0.000*\"machine\"')\n"
     ]
    }
   ],
   "source": [
    "#It is important to mention here that LDA is an unsupervised learning algorithm and \n",
    "#in real-world problems, you will not know about the topics in the dataset beforehand. \n",
    "#You will simply be given a corpus, the topics will be created using LDA \n",
    "#and then the names of the topics are up to you.\n",
    "\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling via LSI - Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T23:52:02.762011Z",
     "start_time": "2023-11-28T23:52:02.694051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.317*\"intelligence\" + 0.291*\"machine\" + 0.281*\"learning\" + 0.272*\"problem\" + 0.246*\"artificial\" + 0.183*\"network\" + 0.145*\"system\" + 0.143*\"knowledge\" + 0.138*\"search\" + 0.137*\"program\"')\n",
      "(1, '-0.746*\"cruise\" + -0.332*\"vehicle\" + -0.155*\"francisco\" + -0.128*\"driving\" + -0.115*\"announced\" + -0.115*\"service\" + -0.114*\"company\" + -0.114*\"autonomous\" + -0.101*\"satellite\" + -0.099*\"pedestrian\"')\n",
      "(2, '-0.915*\"satellite\" + 0.124*\"cruise\" + -0.089*\"launch\" + -0.083*\"communication\" + -0.082*\"launched\" + -0.082*\"united\" + -0.074*\"observation\" + -0.062*\"rocket\" + -0.055*\"sputnik\" + -0.049*\"scientific\"')\n",
      "(3, '-0.651*\"eiffel\" + -0.206*\"second\" + -0.184*\"french\" + -0.148*\"structure\" + -0.142*\"exposition\" + -0.134*\"tallest\" + -0.116*\"engineer\" + -0.109*\"restaurant\" + -0.109*\"construction\" + -0.108*\"france\"')\n"
     ]
    }
   ],
   "source": [
    "lsi_model = LsiModel(gensim_corpus, num_topics=4, id2word=gensim_dictionary)\n",
    "topics = lsi_model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-Based Matching - spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T00:13:09.454751Z",
     "start_time": "2023-11-29T00:13:08.664751Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_trf\n",
    "# !pip install spacy-transformers\n",
    "\n",
    "import spacy\n",
    "\n",
    "import spacy_transformers\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "m_tool = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T00:13:09.473567Z",
     "start_time": "2023-11-29T00:13:09.454751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12825528024649263697, 1, 6), (12825528024649263697, 13, 16), (12825528024649263697, 21, 22), (12825528024649263697, 29, 31)]\n",
      "12825528024649263697 QBF 1 6 quick-brown-fox\n",
      "12825528024649263697 QBF 13 16 quick brown fox\n",
      "12825528024649263697 QBF 21 22 quickbrownfox\n",
      "12825528024649263697 QBF 29 31 quick brownfox\n"
     ]
    }
   ],
   "source": [
    "p1 = [{'LOWER': 'quickbrownfox'}]\n",
    "p2 = [{'LOWER': 'quick'}, {'IS_PUNCT': True}, {'LOWER': 'brown'}, {'IS_PUNCT': True}, {'LOWER': 'fox'}]\n",
    "p3 = [{'LOWER': 'quick'}, {'LOWER': 'brown'}, {'LOWER': 'fox'}]\n",
    "p4 = [{'LOWER': 'quick'}, {'LOWER': 'brownfox'}]\n",
    "\n",
    "m_tool.add('QBF', [p1,p2,p3,p4])\n",
    "\n",
    "sentence = nlp(u'The quick-brown-fox jumps over the lazy dog. The quick brown fox eats well. \\\n",
    "               the quickbrownfox is dead. the dog misses the quick brownfox')\n",
    "\n",
    "phrase_matches = m_tool(sentence)\n",
    "print(phrase_matches )\n",
    "\n",
    "for match_id, start, end in phrase_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  \n",
    "    span = sentence[start:end]                   \n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase-Based Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T00:04:00.724388Z",
     "start_time": "2023-11-29T00:03:57.826708Z"
    }
   },
   "outputs": [],
   "source": [
    "import bs4 as bs  \n",
    "import urllib.request  \n",
    "import re  \n",
    "import nltk\n",
    "\n",
    "scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')  \n",
    "article = scrapped_data.read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:  \n",
    "    article_text += p.text\n",
    "    \n",
    "    \n",
    "processed_article = article_text.lower()  \n",
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )  \n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T00:04:01.760779Z",
     "start_time": "2023-11-29T00:04:00.724388Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5530044837203964789 AI 1086 1088 machine learning\n",
      "5530044837203964789 AI 1119 1121 machine learning\n",
      "5530044837203964789 AI 1231 1233 machine learning\n",
      "5530044837203964789 AI 2444 2446 machine learning\n",
      "5530044837203964789 AI 2968 2970 machine learning\n",
      "5530044837203964789 AI 2987 2989 machine learning\n",
      "5530044837203964789 AI 3462 3464 machine learning\n",
      "5530044837203964789 AI 3501 3503 machine learning\n",
      "5530044837203964789 AI 3973 3975 machine learning\n",
      "5530044837203964789 AI 4046 4048 machine learning\n",
      "5530044837203964789 AI 4383 4385 machine learning\n",
      "5530044837203964789 AI 4421 4423 machine learning\n",
      "5530044837203964789 AI 4454 4456 machine learning\n",
      "5530044837203964789 AI 4736 4738 machine learning\n",
      "5530044837203964789 AI 4794 4796 machine learning\n",
      "5530044837203964789 AI 5088 5089 robots\n",
      "5530044837203964789 AI 5185 5187 machine learning\n",
      "5530044837203964789 AI 5255 5257 machine learning\n",
      "5530044837203964789 AI 5387 5388 robots\n",
      "5530044837203964789 AI 7216 7218 machine learning\n",
      "5530044837203964789 AI 7302 7304 machine learning\n",
      "5530044837203964789 AI 7647 7649 machine learning\n",
      "5530044837203964789 AI 7674 7676 machine learning\n",
      "5530044837203964789 AI 8920 8921 robots\n"
     ]
    }
   ],
   "source": [
    "#Create Phrase Matcher Object\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "phrases = ['machine learning', 'robots', 'intelligent agents']\n",
    "\n",
    "patterns = [nlp(text) for text in phrases]\n",
    "\n",
    "phrase_matcher.add('AI', None, *patterns)\n",
    "\n",
    "sentence = nlp(processed_article)\n",
    "\n",
    "matched_phrases = phrase_matcher(sentence)\n",
    "\n",
    "for match_id, start, end in matched_phrases:\n",
    "    string_id = nlp.vocab.strings[match_id]  \n",
    "    span = sentence[start:end]                   \n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T00:11:01.970286Z",
     "start_time": "2023-11-29T00:11:01.966061Z"
    }
   },
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Built-In Gensim Models and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T00:11:02.843292Z",
     "start_time": "2023-11-29T00:11:02.839650Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T00:11:03.130770Z",
     "start_time": "2023-11-29T00:11:03.120744Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_list = [\n",
    "#     'fasttext-wiki-news-subwords-300', 'glove-twitter-100',\n",
    "#     'glove-twitter-200', 'glove-twitter-25', 'glove-twitter-50',\n",
    "#     'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200',\n",
    "#     'glove-wiki-gigaword-300', 'glove-wiki-gigaword-50',\n",
    "#     'conceptnet-numberbatch-17-06-300'\n",
    "# ]\n",
    "\n",
    "# w2v_embedding = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "# w2v_embedding.save(\n",
    "#     r'/mnt/c/Users/handy019/OneDrive - University of South Australia/Python/3 webscrapping/4 do it again/gensim pre-trained model/fasttext-wiki-news-subwords-300.d2v'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T00:11:03.712088Z",
     "start_time": "2023-11-29T00:11:03.420619Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v_embedding = KeyedVectors.load(\n",
    "    r'F:\\All data and docs\\Python\\3 webscrapping\\4 do it again\\gensim pre-trained model\\glove-wiki-gigaword-300.d2v'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T00:11:04.148567Z",
     "start_time": "2023-11-29T00:11:03.940036Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('photovoltaics', 0.6885057091712952),\n",
       " ('solar', 0.6298309564590454),\n",
       " ('thin-film', 0.5802013278007507),\n",
       " ('pv', 0.5194124579429626),\n",
       " ('geothermal', 0.49324947595596313)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_embedding.most_similar(\"photovoltaic\",topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T00:11:04.436400Z",
     "start_time": "2023-11-29T00:11:04.413863Z"
    }
   },
   "outputs": [],
   "source": [
    "list1 = [x[0] for x in w2v_embedding.most_similar(\"decision-making\",topn=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T00:11:10.972923Z",
     "start_time": "2023-11-29T00:11:10.968048Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['policy-making',\n",
       " 'decisionmaking',\n",
       " 'problem-solving',\n",
       " 'decision-makers',\n",
       " 'organizational',\n",
       " 'processes',\n",
       " 'participatory',\n",
       " 'organisational',\n",
       " 'participative',\n",
       " 'lawmaking',\n",
       " 'policymaking',\n",
       " 'budgeting',\n",
       " 'top-down',\n",
       " 'normative',\n",
       " 'governance',\n",
       " 'deliberative',\n",
       " 'evidence-based',\n",
       " 'decisions',\n",
       " 'decentralized',\n",
       " 'bottom-up',\n",
       " 'centralized',\n",
       " 'socialization',\n",
       " 'reasoning',\n",
       " 'conceptualization',\n",
       " 'facilitates',\n",
       " 'methodology',\n",
       " 'competence',\n",
       " 'methodologies',\n",
       " 'outcomes',\n",
       " 'hierarchical',\n",
       " 'contexts',\n",
       " 'heuristics',\n",
       " 'rational',\n",
       " 'delegated',\n",
       " 'stakeholders',\n",
       " 'rationality',\n",
       " 'day-to-day',\n",
       " 'collaborative',\n",
       " 'subjective',\n",
       " 'consensus-based',\n",
       " 'discourse',\n",
       " 'competences',\n",
       " 'optimal',\n",
       " 'know-how',\n",
       " 'cognition',\n",
       " 'competencies',\n",
       " 'workflows',\n",
       " 'interpersonal',\n",
       " 'decentralization',\n",
       " 'co-ordinate']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
